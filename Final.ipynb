{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9f06ced-ff11-4f27-9111-286c2c5dabe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 10:25:11,468 - INFO - Number of CPU cores available: 12\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Intra op parallelism cannot be modified after initialization.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of CPU cores available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Configure TensorFlow to use all available CPU cores\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mthreading\u001b[38;5;241m.\u001b[39mset_intra_op_parallelism_threads(num_cores)\n\u001b[0;32m     32\u001b[0m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mthreading\u001b[38;5;241m.\u001b[39mset_inter_op_parallelism_threads(num_cores)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(window, sampling_rate):\n",
      "File \u001b[1;32m~\\.conda\\envs\\project\\Lib\\site-packages\\tensorflow\\python\\framework\\config.py:129\u001b[0m, in \u001b[0;36mset_intra_op_parallelism_threads\u001b[1;34m(num_threads)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.threading.set_intra_op_parallelism_threads\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_intra_op_parallelism_threads\u001b[39m(num_threads):\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Set number of threads used within an individual op for parallelism.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m  Certain operations like matrix multiplication and reductions can utilize\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    num_threads: Number of parallel threads\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m   context\u001b[38;5;241m.\u001b[39mcontext()\u001b[38;5;241m.\u001b[39mintra_op_parallelism_threads \u001b[38;5;241m=\u001b[39m num_threads\n",
      "File \u001b[1;32m~\\.conda\\envs\\project\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:2065\u001b[0m, in \u001b[0;36mContext.intra_op_parallelism_threads\u001b[1;34m(self, num_threads)\u001b[0m\n\u001b[0;32m   2062\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2065\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2066\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntra op parallelism cannot be modified after initialization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intra_op_parallelism_threads \u001b[38;5;241m=\u001b[39m num_threads\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Intra op parallelism cannot be modified after initialization."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime \n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from scipy import signal, stats\n",
    "from obspy.signal.trigger import classic_sta_lta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Determine the number of CPU cores to use\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "logging.info(f\"Number of CPU cores available: {num_cores}\")\n",
    "\n",
    "# Configure TensorFlow to use all available CPU cores\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_cores)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_cores)\n",
    "\n",
    "def extract_features(window, sampling_rate):\n",
    "    \"\"\"Extract advanced features from the seismic data window.\"\"\"\n",
    "    fft = np.fft.fft(window)\n",
    "    freqs = np.fft.fftfreq(len(window), 1/sampling_rate)\n",
    "    power_spectrum = np.abs(fft)**2\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(window),\n",
    "        'std': np.std(window),\n",
    "        'max': np.max(window),\n",
    "        'min': np.min(window),\n",
    "        'median': np.median(window),\n",
    "        'skewness': stats.skew(window),\n",
    "        'kurtosis': stats.kurtosis(window),\n",
    "        'energy': np.sum(window**2),\n",
    "        'rms': np.sqrt(np.mean(window**2)),\n",
    "        'zero_crossings': np.sum(np.diff(np.sign(window)) != 0),\n",
    "        'sta_lta': classic_sta_lta(window, int(0.5 * sampling_rate), int(10 * sampling_rate)).max(),\n",
    "        'dominant_freq': freqs[np.argmax(power_spectrum)],\n",
    "        'spectral_centroid': np.sum(freqs * power_spectrum) / np.sum(power_spectrum),\n",
    "        'spectral_bandwidth': np.sqrt(np.sum(((freqs - np.sum(freqs * power_spectrum) / np.sum(power_spectrum))**2) * power_spectrum) / np.sum(power_spectrum)),\n",
    "        'spectral_rolloff': freqs[np.where(np.cumsum(power_spectrum) >= 0.85 * np.sum(power_spectrum))[0][0]]\n",
    "    }\n",
    "\n",
    "def create_model(input_shape, n_features):\n",
    "       # Current complex model architecture\n",
    "       ts_input = Input(shape=input_shape)\n",
    "       x = Conv1D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(1e-4))(ts_input)\n",
    "       x = BatchNormalization()(x)\n",
    "       x = MaxPooling1D(2)(x)\n",
    "       x = Conv1D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "       x = BatchNormalization()(x)\n",
    "       x = MaxPooling1D(2)(x)\n",
    "       x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "       x = Dropout(0.3)(x)\n",
    "       x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "       x = Dropout(0.3)(x)\n",
    "       attention = Attention()([x, x])\n",
    "       x = tf.keras.layers.GlobalAveragePooling1D()(attention)\n",
    "       feat_input = Input(shape=(n_features,))\n",
    "       combined = Concatenate()([x, feat_input])\n",
    "       x = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(combined)\n",
    "       x = BatchNormalization()(x)\n",
    "       x = Dropout(0.3)(x)\n",
    "       x = Dense(32, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "       x = BatchNormalization()(x)\n",
    "       x = Dropout(0.3)(x)\n",
    "       output = Dense(2, activation='softmax')(x)\n",
    "       \n",
    "       # Suggested simpler model architecture\n",
    "       # ts_input = Input(shape=input_shape)\n",
    "       # x = Conv1D(32, 3, activation='relu', padding='same')(ts_input)\n",
    "       # x = MaxPooling1D(2)(x)\n",
    "       # x = Bidirectional(LSTM(32, return_sequences=False))(x)\n",
    "       # feat_input = Input(shape=(n_features,))\n",
    "       # combined = Concatenate()([x, feat_input])\n",
    "       # x = Dense(32, activation='relu')(combined)\n",
    "       # x = Dropout(0.3)(x)\n",
    "       # output = Dense(2, activation='softmax')(x)\n",
    "       \n",
    "       model = Model(inputs=[ts_input, feat_input], outputs=output)\n",
    "       model.compile(optimizer=Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "       return model\n",
    "\n",
    "def process_seismic_file(mseed_file, catalog_file, time_column, window_size=60, overlap=0.5):\n",
    "    \"\"\"Process a single seismic data file and return windowed data with labels.\"\"\"\n",
    "    try:\n",
    "        st = read(mseed_file)\n",
    "        tr = st[0]\n",
    "        data = tr.data\n",
    "        sampling_rate = tr.stats.sampling_rate\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading mseed file {mseed_file}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        cat = pd.read_csv(catalog_file)\n",
    "        event_times = pd.to_datetime(cat[time_column])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading catalog file: {e}\")\n",
    "        if 'cat' in locals():\n",
    "            logging.info(f\"Available columns: {cat.columns.tolist()}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    trace_start_time = tr.stats.starttime.datetime\n",
    "\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    overlap_samples = int(window_samples * overlap)\n",
    "    stride = window_samples - overlap_samples\n",
    "\n",
    "    windows, labels, features = [], [], []\n",
    "\n",
    "    for i in range(0, len(data) - window_samples, stride):\n",
    "        window = data[i:i + window_samples]\n",
    "        window_features = extract_features(window, sampling_rate)\n",
    "\n",
    "        window_start_time = trace_start_time + datetime.timedelta(seconds=i / sampling_rate)\n",
    "        window_end_time = window_start_time + datetime.timedelta(seconds=window_size)\n",
    "        is_event = any((window_start_time <= event <= window_end_time) for event in event_times)\n",
    "\n",
    "        windows.append(window)\n",
    "        features.append(window_features)\n",
    "        labels.append(1 if is_event else 0)\n",
    "\n",
    "    return np.array(windows), features, np.array(labels)\n",
    "\n",
    "def prepare_data(data_directory, catalog_file, time_column):\n",
    "    \"\"\"Prepare data for training and evaluation using parallel processing.\"\"\"\n",
    "    mseed_files = [os.path.join(data_directory, f) for f in os.listdir(data_directory) if f.endswith('.mseed')]\n",
    "    \n",
    "    # Use joblib to parallelize file processing\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_seismic_file)(mseed_file, catalog_file, time_column)\n",
    "        for mseed_file in tqdm(mseed_files, desc=\"Processing files\")\n",
    "    )\n",
    "\n",
    "    all_windows, all_features, all_labels = [], [], []\n",
    "    for windows, features, labels in results:\n",
    "        if windows is not None:\n",
    "            all_windows.append(windows)\n",
    "            all_features.extend(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    if not all_windows:\n",
    "        raise RuntimeError(\"No data could be processed successfully\")\n",
    "\n",
    "    all_windows = np.vstack(all_windows)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    feature_names = list(all_features[0].keys())\n",
    "    feature_array = np.array([[f[name] for name in feature_names] for f in all_features])\n",
    "\n",
    "    return all_windows, feature_array, all_labels, feature_names\n",
    "\n",
    "def train_and_evaluate(data_directory, catalog_file):\n",
    "    \"\"\"Train and evaluate the model with improved techniques and CPU optimization.\"\"\"\n",
    "    logging.info(\"Examining catalog file structure...\")\n",
    "    cat = pd.read_csv(catalog_file)\n",
    "    if cat is None:\n",
    "        return None, None, None\n",
    "\n",
    "    possible_time_columns = ['time', 'Time', 'timestamp', 'Timestamp', 'event_time', 'Event_Time', 'DateTime', 'time_abs']\n",
    "    time_column = next((col for col in cat.columns if any(time_name.lower() in col.lower() for time_name in possible_time_columns)), None)\n",
    "\n",
    "    if time_column is None:\n",
    "        logging.error(\"Could not automatically identify time column. Available columns are:\")\n",
    "        logging.error(cat.columns.tolist())\n",
    "        return None, None, None\n",
    "\n",
    "    logging.info(f\"Using '{time_column}' as the time column.\")\n",
    "\n",
    "    logging.info(\"Preparing data...\")\n",
    "    try:\n",
    "        windows, features, labels, feature_names = prepare_data(data_directory, catalog_file, time_column)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error preparing data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    histories = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(windows)):\n",
    "        logging.info(f\"\\nFold {fold+1}\")\n",
    "        \n",
    "        X_train_ts, X_val_ts = windows[train_idx], windows[val_idx]\n",
    "        X_train_feat, X_val_feat = scaled_features[train_idx], scaled_features[val_idx]\n",
    "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
    "        \n",
    "        smote = SMOTE(random_state=42, n_jobs=num_cores)  # Use all CPU cores for SMOTE\n",
    "        X_train_feat_resampled, y_train_resampled = smote.fit_resample(X_train_feat, y_train)\n",
    "        \n",
    "        X_train_ts_flat = X_train_ts.reshape(X_train_ts.shape[0], -1)\n",
    "        X_train_ts_resampled, _ = smote.fit_resample(X_train_ts_flat, y_train)\n",
    "        X_train_ts_resampled = X_train_ts_resampled.reshape(-1, X_train_ts.shape[1], 1)\n",
    "        \n",
    "        model = create_model(input_shape=(windows.shape[1], 1), n_features=features.shape[1])\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint(f'model_fold_{fold+1}.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "        \n",
    "        history = model.fit(\n",
    "            [X_train_ts_resampled, X_train_feat_resampled], \n",
    "            y_train_resampled,\n",
    "            validation_data=([X_val_ts[..., np.newaxis], X_val_feat], y_val),\n",
    "            epochs=10,\n",
    "            batch_size=32 * num_cores,\n",
    "            callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
    "        )\n",
    "        histories.append(history)\n",
    "        \n",
    "        y_pred_proba = model.predict([X_val_ts[..., np.newaxis], X_val_feat])\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        logging.info(\"\\nValidation Set Performance:\")\n",
    "        logging.info(classification_report(y_val, y_pred))\n",
    "        logging.info(f\"F1 Score: {f1_score(y_val, y_pred, average='weighted')}\")\n",
    "        \n",
    "        # Calculate Precision-Recall AUC\n",
    "        precision, recall, _ = precision_recall_curve(y_val, y_pred_proba[:, 1])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        logging.info(f\"Precision-Recall AUC: {pr_auc}\")\n",
    "\n",
    "    try:\n",
    "        model.save('lunar_seismic_model.keras')\n",
    "        np.save('feature_names.npy', feature_names)\n",
    "        logging.info(\"Model and feature names saved successfully\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving model: {e}\")\n",
    "\n",
    "    return model, scaler, feature_names\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = './data/lunar/training/data/S12_GradeA'\n",
    "    catalog_file = './data/lunar/training/catalogs/apollo12_catalog_GradeA_final.csv'\n",
    "    \n",
    "    model, scaler, feature_names = train_and_evaluate(data_directory, catalog_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032680f-1e21-418e-b5d8-cd464a2ea510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
