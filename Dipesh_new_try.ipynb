{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31d7762-3c67-49cc-baf3-b0c7bbc302ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog loaded. Shape: (76, 5)\n",
      "                                 filename time_abs(%Y-%m-%dT%H:%M:%S.%f)  \\\n",
      "0  xa.s12.00.mhz.1970-01-19HR00_evid00002     1970-01-19T20:25:00.000000   \n",
      "1  xa.s12.00.mhz.1970-03-25HR00_evid00003     1970-03-25T03:32:00.000000   \n",
      "2  xa.s12.00.mhz.1970-03-26HR00_evid00004     1970-03-26T20:17:00.000000   \n",
      "3  xa.s12.00.mhz.1970-04-25HR00_evid00006     1970-04-25T01:14:00.000000   \n",
      "4  xa.s12.00.mhz.1970-04-26HR00_evid00007     1970-04-26T14:29:00.000000   \n",
      "\n",
      "   time_rel(sec)       evid    mq_type  \n",
      "0        73500.0  evid00002  impact_mq  \n",
      "1        12720.0  evid00003  impact_mq  \n",
      "2        73020.0  evid00004  impact_mq  \n",
      "3         4440.0  evid00006  impact_mq  \n",
      "4        52140.0  evid00007    deep_mq  \n",
      "Loading file: ./data/lunar/training/data/S12_GradeA/xa.s12.00.mhz.1970-01-19HR00_evid00002.mseed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dipes\\.conda\\envs\\project\\Lib\\site-packages\\obspy\\signal\\filter.py:62: UserWarning: Selected high corner frequency (3.3125) of bandpass is at or above Nyquist (3.3125). Applying a high-pass instead.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Prepare the dataset\u001b[39;00m\n\u001b[0;32m    106\u001b[0m data_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/lunar/training/data/S12_GradeA/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 107\u001b[0m X, y \u001b[38;5;241m=\u001b[39m prepare_dataset(catalog, data_directory)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset prepared. Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# 5. Exploratory Data Analysis\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 91\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[1;34m(catalog, data_directory, window_size)\u001b[0m\n\u001b[0;32m     89\u001b[0m end_time \u001b[38;5;241m=\u001b[39m event_time \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(seconds\u001b[38;5;241m=\u001b[39mwindow_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     90\u001b[0m processed_stream \u001b[38;5;241m=\u001b[39m preprocess_seismic_data(stream, start_time, end_time)\n\u001b[1;32m---> 91\u001b[0m features \u001b[38;5;241m=\u001b[39m extract_features(processed_stream[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     92\u001b[0m X\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m     93\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 1 for event\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 62\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(trace)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Using PyWavelets for wavelet transform\u001b[39;00m\n\u001b[0;32m     61\u001b[0m widths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m---> 62\u001b[0m coeffs \u001b[38;5;241m=\u001b[39m pywt\u001b[38;5;241m.\u001b[39mcwt(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcmor1.5-1.0\u001b[39m\u001b[38;5;124m'\u001b[39m, widths)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     63\u001b[0m wavelet_energy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(coeffs)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     64\u001b[0m wavelet_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(wavelet_energy)\n",
      "File \u001b[1;32m~\\.conda\\envs\\project\\Lib\\site-packages\\pywt\\_cwt.py:117\u001b[0m, in \u001b[0;36mcwt\u001b[1;34m(data, scales, wavelet, sampling_period, method, axis)\u001b[0m\n\u001b[0;32m    115\u001b[0m dt_cplx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(dt, np\u001b[38;5;241m.\u001b[39mcomplex64)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wavelet, (ContinuousWavelet, Wavelet)):\n\u001b[1;32m--> 117\u001b[0m     wavelet \u001b[38;5;241m=\u001b[39m DiscreteContinuousWavelet(wavelet)\n\u001b[0;32m    119\u001b[0m scales \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(scales)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scales \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32mpywt\\\\_extensions\\\\_pywt.pyx:318\u001b[0m, in \u001b[0;36mpywt._extensions._pywt.DiscreteContinuousWavelet\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from obspy import read, UTCDateTime\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import pywt  # Using PyWavelets instead of deprecated scipy.signal.cwt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Data Loading\n",
    "def load_catalog(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def load_seismic_data(file_path):\n",
    "    return read(file_path)\n",
    "\n",
    "# Load the catalog\n",
    "cat_directory = './data/lunar/training/catalogs/'\n",
    "cat_file = cat_directory + 'apollo12_catalog_GradeA_final.csv'\n",
    "catalog = load_catalog(cat_file)\n",
    "\n",
    "print(\"Catalog loaded. Shape:\", catalog.shape)\n",
    "print(catalog.head())\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "def preprocess_seismic_data(stream, start_time, end_time):\n",
    "    start_time = UTCDateTime(start_time.isoformat())\n",
    "    end_time = UTCDateTime(end_time.isoformat())\n",
    "    \n",
    "    stream = stream.trim(starttime=start_time, endtime=end_time)\n",
    "    stream.detrend('linear')\n",
    "    stream.filter('bandpass', freqmin=0.1, freqmax=3.3125)  # Adjusted to be below Nyquist\n",
    "    return stream\n",
    "\n",
    "# 3. Advanced Feature Extraction\n",
    "def extract_features(trace):\n",
    "    data = trace.data\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    max_amp = np.max(np.abs(data))\n",
    "    rms = np.sqrt(np.mean(data**2))\n",
    "    kurtosis = np.mean((data - mean)**4) / std**4\n",
    "    \n",
    "    fft = np.fft.fft(data)\n",
    "    freq = np.fft.fftfreq(len(data), d=trace.stats.delta)\n",
    "    power = np.abs(fft)**2\n",
    "    dominant_freq = freq[np.argmax(power)]\n",
    "    spectral_centroid = np.sum(freq * power) / np.sum(power)\n",
    "    \n",
    "    # Using PyWavelets for wavelet transform\n",
    "    widths = np.arange(1, 128)\n",
    "    coeffs = pywt.cwt(data, 'cmor1.5-1.0', widths)[0]\n",
    "    wavelet_energy = np.sum(np.abs(coeffs)**2, axis=0)\n",
    "    wavelet_mean = np.mean(wavelet_energy)\n",
    "    wavelet_std = np.std(wavelet_energy)\n",
    "    \n",
    "    return [mean, std, max_amp, rms, kurtosis, dominant_freq, spectral_centroid, wavelet_mean, wavelet_std]\n",
    "\n",
    "# 4. Prepare Dataset\n",
    "def prepare_dataset(catalog, data_directory, window_size=600):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _, row in catalog.iterrows():\n",
    "        file_name = row['filename']\n",
    "        event_time = pd.to_datetime(row['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "        \n",
    "        mseed_file = os.path.join(data_directory, file_name + '.mseed')\n",
    "        print(\"Loading file:\", mseed_file)  # Debugging line\n",
    "        \n",
    "        if not os.path.isfile(mseed_file):\n",
    "            print(f\"File not found: {mseed_file}. Skipping this entry.\")\n",
    "            continue\n",
    "        \n",
    "        stream = load_seismic_data(mseed_file)\n",
    "        \n",
    "        # Event window\n",
    "        start_time = event_time - pd.Timedelta(seconds=window_size//2)\n",
    "        end_time = event_time + pd.Timedelta(seconds=window_size//2)\n",
    "        processed_stream = preprocess_seismic_data(stream, start_time, end_time)\n",
    "        features = extract_features(processed_stream[0])\n",
    "        X.append(features)\n",
    "        y.append(1)  # 1 for event\n",
    "        \n",
    "        # Non-event window\n",
    "        non_event_start = start_time - pd.Timedelta(seconds=window_size)\n",
    "        non_event_end = start_time\n",
    "        non_event_stream = preprocess_seismic_data(stream, non_event_start, non_event_end)\n",
    "        non_event_features = extract_features(non_event_stream[0])\n",
    "        X.append(non_event_features)\n",
    "        y.append(0)  # 0 for non-event\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare the dataset\n",
    "data_directory = './data/lunar/training/data/S12_GradeA/'\n",
    "X, y = prepare_dataset(catalog, data_directory)\n",
    "\n",
    "print(\"Dataset prepared. Shape:\", X.shape)\n",
    "\n",
    "# 5. Exploratory Data Analysis\n",
    "def plot_feature_distributions(X, y):\n",
    "    feature_names = ['Mean', 'Std Dev', 'Max Amplitude', 'RMS', 'Kurtosis', \n",
    "                     'Dominant Frequency', 'Spectral Centroid', 'Wavelet Mean', 'Wavelet Std']\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['Event'] = y\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        sns.histplot(data=df, x=feature, hue='Event', kde=True, ax=axes[i])\n",
    "        axes[i].set_title(feature)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_distributions(X, y)\n",
    "\n",
    "# 6. Model Preparation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for CNN\n",
    "X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# 7. Build and Train Model\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, 3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(128, 3, activation='relu'),\n",
    "        MaxPooling1D(2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model((X_train_reshaped.shape[1], 1))\n",
    "print(model.summary())\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)\n",
    "\n",
    "# 8. Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "evaluate_model(model, X_test_reshaped, y_test)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. Feature Importance\n",
    "def plot_feature_importance(model, feature_names):\n",
    "    # For CNN, we'll use the weights of the first convolutional layer\n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "    importance = np.sum(np.abs(weights), axis=(0, 1))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(feature_names, importance)\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "feature_names = ['Mean', 'Std Dev', 'Max Amplitude', 'RMS', 'Kurtosis', \n",
    "                 'Dominant Frequency', 'Spectral Centroid', 'Wavelet Mean', 'Wavelet Std']\n",
    "plot_feature_importance(model, feature_names)\n",
    "\n",
    "# 10. Prediction on New Data\n",
    "def predict_event(model, scaler, file_path, event_time, window_size=600):\n",
    "    stream = load_seismic_data(file_path)\n",
    "    start_time = event_time - pd.Timedelta(seconds=window_size//2)\n",
    "    end_time = event_time + pd.Timedelta(seconds=window_size//2)\n",
    "    processed_stream = preprocess_seismic_data(stream, start_time, end_time)\n",
    "    \n",
    "    features = extract_features(processed_stream[0])\n",
    "    features_scaled = scaler.transform([features])\n",
    "    features_reshaped = features_scaled.reshape(features_scaled.shape[0], features_scaled.shape[1], 1)\n",
    "    \n",
    "    prediction = model.predict(features_reshaped)\n",
    "    return prediction[0][0]\n",
    "\n",
    "# Example usage\n",
    "# file_path = './data/lunar/training/data/S12_GradeA/example.mseed'\n",
    "# event_time = pd.to_datetime('1970-01-19T20:25:00.000000')\n",
    "# prediction = predict_event(model, scaler, file_path, event_time)\n",
    "# print(f\"Prediction for event: {'Event' if prediction > 0.5 else 'Non-Event'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893248e6-432f-4eef-a02c-3ea5e8b54b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
